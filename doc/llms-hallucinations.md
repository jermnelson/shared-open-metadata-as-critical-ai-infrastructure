## Hallucinations of Large Language Models
A persistent problem, even for the frontier Large Language Models from OpenAI, Antropic, Google, and
Deep Seek, is the tendency for these models to fabricate facts in an increasingly pervasive fashion.
Because Large Language Models are transformer-based neural networks that at their core are predicting
the next series of tokens based on its context, this variability means that hallucinations are 
fundamental to this technology. 

In a recent paper[^1], Open AI researchers assert that hallucinations
are a consequence of the how models are trained and awarded for guessing on benchmarks and tests and that
the it isn't necessarily because of the nature of the LLM architecture.
 

The commercial vendors of these LLMs all attempt to minimize 
the hallucinations of their models, with varying success, including 

[^1]: [Why Language Models Hallucinate](https://openai.com/index/why-language-models-hallucinate/)
